{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQP1XNUJxbV9"
   },
   "source": [
    "# Tutorial D2: Grasp Prediction\n",
    "\n",
    "## Tasks\n",
    "\n",
    "But first: Copy notebook to the drive (button should be just next to the +Copy +Text)\n",
    "Goal: Train a custom-made neural network to predict whether a grasp will be successful based on two tactile images. (binary classification)\n",
    "\n",
    "### Setup\n",
    "\n",
    "1. Create conda venv and install Tacto on your machine\n",
    "2. Clone\n",
    "```\n",
    "https://github.com/facebookresearch/tacto/\n",
    "```\n",
    "3. Call\n",
    "```\n",
    "pip install -r tacto/requirements/examples.txt\n",
    "```\n",
    "3. Go to **experiments/grasp_stability**\n",
    "4. Call\n",
    "```\n",
    "pip install scipyplot deepdish torch torchvision\n",
    "```\n",
    "\n",
    "### Collect data\n",
    "1. Default script will collects N x 100 samples. How fast is your machine?\n",
    "  \n",
    "  a. It's fast and I have over 100GB free:\n",
    "  ```\n",
    "  python grasp_data_collection.py\n",
    "  ```\n",
    "  b. It is not that fast, and I have less memory:\n",
    "  ```\n",
    "  295    print(\"\\rsample {}\".format(log.id * log.batch_size + len(log.dataList)),end=\"\")\n",
    "  296\n",
    "  297     # print(\"\\rsample {}\".format(log.id), end=\"\")\n",
    "  298\n",
    "  299    if log.id > 2000: # N is 2000, for N = 100 dataset size is ~5.7GB\n",
    "            break\n",
    "  ```\n",
    "\n",
    "  ```\n",
    "  112    data = {\n",
    "  113       \"tactileColorL\": tactileColorL,\n",
    "  114       \"tactileColorR\": tactileColorR,\n",
    "  115       # \"tactileDepthL\": tactileDepthL,\n",
    "  116       # \"tactileDepthR\": tactileDepthR,\n",
    "  117       # \"visionColor\": visionColor,\n",
    "  118      # \"visionDepth\": visionDepth,\n",
    "  119       # \"gripForce\": gripForce,\n",
    "  120       # \"normalForce\": normalForce,\n",
    "  121       \"label\": label,\n",
    "  122   }\n",
    "  ```\n",
    "\n",
    "2. Zip **data** folder and upload it to **Google Drive** or directly\n",
    "\n",
    "## Implement\n",
    "1. MyModel class\n",
    "2. Training loop\n",
    "3. Save model\n",
    "\n",
    "- Use materials from the previous lectures and PyTorch documentation Links:\n",
    "- https://colab.research.google.com/drive/1KyrXe6kErnAYTuFKsRnvDhRLS4pBAiXa?usp=sharing\n",
    "- https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT5LtUSFyI-x"
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fXnpe2deFD7N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepdish in /home/h9/boje081f/.local/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: numpy in /software/util/JupyterLab/alpha/share/pytorch_v2/lib/python3.10/site-packages (from deepdish) (1.26.4)\n",
      "Requirement already satisfied: scipy in /software/util/JupyterLab/alpha/share/pytorch_v2/lib/python3.10/site-packages (from deepdish) (1.12.0)\n",
      "Requirement already satisfied: tables in /home/h9/boje081f/.local/lib/python3.10/site-packages (from deepdish) (3.10.1)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /home/h9/boje081f/.local/lib/python3.10/site-packages (from tables->deepdish) (2.10.2)\n",
      "Requirement already satisfied: packaging in /software/util/JupyterLab/alpha/share/pytorch_v2/lib/python3.10/site-packages (from tables->deepdish) (23.2)\n",
      "Requirement already satisfied: py-cpuinfo in /home/h9/boje081f/.local/lib/python3.10/site-packages (from tables->deepdish) (9.0.0)\n",
      "Requirement already satisfied: blosc2>=2.3.0 in /home/h9/boje081f/.local/lib/python3.10/site-packages (from tables->deepdish) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /software/util/JupyterLab/alpha/share/pytorch_v2/lib/python3.10/site-packages (from tables->deepdish) (4.8.0)\n",
      "Requirement already satisfied: ndindex>=1.4 in /home/h9/boje081f/.local/lib/python3.10/site-packages (from blosc2>=2.3.0->tables->deepdish) (1.9.2)\n",
      "Requirement already satisfied: msgpack in /software/rome/r23.04/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from blosc2>=2.3.0->tables->deepdish) (1.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepdish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KqJKIimRkhbJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "import h5py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYRrHWOEyOON"
   },
   "source": [
    "# Dataset Class and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "50uPv3QsE8fG"
   },
   "outputs": [],
   "source": [
    "class GraspingDataset(Dataset):\n",
    "    def __init__(self, fileNames, fields=[], transform=None, transformDepth=None):\n",
    "        self.transform = transform\n",
    "        self.transformDepth = transformDepth\n",
    "        self.fileNames = fileNames\n",
    "        self.fields = fields + [\"label\"]\n",
    "        self.numGroup = 100  # data points per file\n",
    "\n",
    "        self.dataList = None\n",
    "        self.dataFileID = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fileNames * self.numGroup)\n",
    "\n",
    "    def load_data(self, idx):\n",
    "        dirName = self.fileNames[idx]\n",
    "        data = {}\n",
    "\n",
    "        for k in self.fields:\n",
    "            fn = dirName.split(\"/\")[-1]\n",
    "\n",
    "            fnk = \"{}_{}.h5\".format(fn, k)\n",
    "\n",
    "            filenamek = os.path.join(dirName, fnk)\n",
    "            d = dd.io.load(filenamek)\n",
    "\n",
    "            data[k] = d\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        fileID = idx // self.numGroup\n",
    "        if fileID != self.dataFileID:\n",
    "            self.dataList = self.load_data(fileID)\n",
    "            self.dataFileID = fileID\n",
    "\n",
    "        sample = {}\n",
    "\n",
    "        data = self.dataList\n",
    "\n",
    "        for k in self.fields:\n",
    "            d = data[k][idx % self.numGroup]\n",
    "\n",
    "            if k in [\"tactileColorL\", \"tactileColorR\", \"visionColor\"]:\n",
    "                d = d[:, :, :3]\n",
    "                # print(k, d.min(), d.max())\n",
    "\n",
    "            if k in [\"tactileDepthL\", \"tactileDepthR\", \"visionDepth\"]:\n",
    "                d = np.dstack([d, d, d])\n",
    "\n",
    "            if k in [\"tactileDepthL\", \"tactileDepthR\"]:\n",
    "                d = d / 0.002 * 255\n",
    "                d = np.clip(d, 0, 255).astype(np.uint8)\n",
    "                # print(\"depth min\", d.min(), \"max\", d.max())\n",
    "\n",
    "            if k in [\"visionDepth\"]:\n",
    "                d = (d * 255).astype(np.uint8)\n",
    "\n",
    "            if k in [\n",
    "                \"tactileColorL\",\n",
    "                \"tactileColorR\",\n",
    "                \"visionColor\",\n",
    "                \"visionDepth\",\n",
    "            ]:\n",
    "                if self.transform:\n",
    "                    d = self.transform(d)\n",
    "\n",
    "            if k in [\n",
    "                \"tactileDepthL\",\n",
    "                \"tactileDepthR\",\n",
    "            ]:\n",
    "                # print(\"before\", d.min(), d.max(), d.mean(), d.std())\n",
    "                d = self.transformDepth(d)\n",
    "                # d = (d + 2) / 0.05\n",
    "                # print(\"after\", d.min(), d.max(), d.mean(), d.std())\n",
    "\n",
    "            sample[k] = d\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qxSKpkPtGP6E"
   },
   "outputs": [],
   "source": [
    "# function that will\n",
    "# - take all samples from rootDir\n",
    "# - divide whole set in N parts\n",
    "# - take i-th for test and rest N-1 for training\n",
    "def load_data(rootDir, K, i):\n",
    "        # K-fold, test the i-th fold, train the rest\n",
    "\n",
    "        fileNames = glob.glob(os.path.join(rootDir, \"*\"))\n",
    "        fileNames = sorted(fileNames)\n",
    "        print(fileNames)\n",
    "\n",
    "        # Split K fold\n",
    "        N = len(fileNames)\n",
    "        n = N // K\n",
    "\n",
    "        idx = list(range(N))\n",
    "        testIdx = idx[n * i : n * (i + 1)]\n",
    "        trainIdx = list(set(idx) - set(testIdx))\n",
    "\n",
    "        trainFileNames = [fileNames[i] for i in trainIdx]\n",
    "        testFileNames = [fileNames[i] for i in testIdx]\n",
    "\n",
    "        trainTransform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                # AddGaussianNoise(0.0, 0.01),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainTransformDepth = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.1,), std=(0.2,)),\n",
    "                # AddGaussianNoise(0.0, 0.01),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create training dataset and dataloader\n",
    "        trainDataset = GraspingDataset(\n",
    "            trainFileNames,\n",
    "            fields=[\"tactileColorL\", \"tactileColorR\"],\n",
    "            transform=trainTransform,\n",
    "            transformDepth=trainTransformDepth,\n",
    "        )\n",
    "        trainLoader = torch.utils.data.DataLoader(\n",
    "            trainDataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True\n",
    "        )\n",
    "\n",
    "        testTransform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        testTransformDepth = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                # transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.1,), std=(0.2,)),\n",
    "                # AddGaussianNoise(0.0, 0.01),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create training dataset and dataloader\n",
    "        testDataset = GraspingDataset(\n",
    "            testFileNames,\n",
    "            fields=[\"tactileColorL\", \"tactileColorR\"],\n",
    "            transform=testTransform,\n",
    "            transformDepth=testTransformDepth,\n",
    "        )\n",
    "        testLoader = torch.utils.data.DataLoader(\n",
    "            testDataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True\n",
    "        )\n",
    "\n",
    "        return trainLoader, testLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wU-023n2iKq"
   },
   "source": [
    "\n",
    "# Task II: Load Dataset\n",
    "- result: you should see two tactile readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CcLYP8ccGce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./grasp/0000000', './grasp/0000001', './grasp/0000002', './grasp/0000003', './grasp/0000004', './grasp/0000005', './grasp/0000006', './grasp/0000007', './grasp/0000008', './grasp/0000009', './grasp/0000010', './grasp/0000011', './grasp/0000012', './grasp/0000013', './grasp/0000014', './grasp/0000015', './grasp/0000016', './grasp/0000017', './grasp/0000018', './grasp/0000019', './grasp/0000020', './grasp/0000021', './grasp/0000022', './grasp/0000023', './grasp/0000024', './grasp/0000025', './grasp/0000026', './grasp/0000027', './grasp/0000028', './grasp/0000029', './grasp/0000030', './grasp/0000031', './grasp/0000032', './grasp/0000033', './grasp/0000034', './grasp/0000035', './grasp/0000036', './grasp/0000037', './grasp/0000038', './grasp/0000039', './grasp/0000040', './grasp/0000041', './grasp/0000042', './grasp/0000043', './grasp/0000044', './grasp/0000045', './grasp/0000046', './grasp/0000047', './grasp/0000048', './grasp/0000049', './grasp/0000050', './grasp/0000051', './grasp/0000052', './grasp/0000053', './grasp/0000054', './grasp/0000055', './grasp/0000056', './grasp/0000057', './grasp/0000058', './grasp/0000059', './grasp/0000060', './grasp/0000061', './grasp/0000062', './grasp/0000063', './grasp/0000064', './grasp/0000065', './grasp/0000066', './grasp/0000067', './grasp/0000068', './grasp/0000069', './grasp/0000070', './grasp/0000071', './grasp/0000072', './grasp/0000073', './grasp/0000074', './grasp/0000075', './grasp/0000076', './grasp/0000077', './grasp/0000078', './grasp/0000079']\n"
     ]
    }
   ],
   "source": [
    "# todo: check if your data is in the right dir\n",
    "rootDir = './grasp'\n",
    "\n",
    "# create dataset loaders (do you know why are we using loaders?)\n",
    "trainLoader, testLoader = load_data(rootDir, 10, 0)\n",
    "\n",
    "# show single test example\n",
    "single_test_batch = next(iter(testLoader))\n",
    "tactileColorL, tactileColorR, label = single_test_batch['tactileColorL'], single_test_batch['tactileColorR'], single_test_batch['label']\n",
    "\n",
    "display_reading_index = 0\n",
    "#plt.title('Label:' + str(label[display_reading_index].item()))\n",
    "#plt.imshow(tactileColorL[display_reading_index,:,:,:].T)\n",
    "#plt.show()\n",
    "\n",
    "#plt.title('Label:' + str(label[display_reading_index].item()))\n",
    "#plt.imshow(tactileColorR[display_reading_index,:,:,:].T)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sDgyKwMj4JpF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(single_test_batch['tactileColorL'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T7hrAaF_CL2"
   },
   "source": [
    "## Task III: Implement custom model and train it\n",
    "- tip 1: all manipulations are performed on tensors, check tensor shape frequently (i.e. print(inputs.shape))\n",
    "- you can use Tutorial A for inspiration https://colab.research.google.com/drive/1KyrXe6kErnAYTuFKsRnvDhRLS4pBAiXa?usp=sharing\n",
    "- one way how it can be done is on slides\n",
    "- for tensor manipulation https://pytorch.org/docs/stable/index.html\n",
    "- take single batch and play with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sXbyoFcH4U-S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "print(len(testLoader))\n",
    "print(len(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xqq8AFMA6u1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  1\n",
      "Current cuda device  0\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(device)/1024**3,1), 'GB')\n",
    "\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PxAr3J9oMwb0"
   },
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# implement network\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float32)\n",
    "        self.vit.classifier = nn.Identity()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.vit.config.hidden_size * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, tactileColorL, tactileColorR) :\n",
    "      featuresL = self.vit(pixel_values=tactileColorL).logits\n",
    "      featuresR = self.vit(pixel_values=tactileColorR).logits\n",
    "      combined_features = torch.cat((featuresL, featuresR), dim=1)\n",
    "      output = self.classifier(combined_features)\n",
    "\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "adi7kMQO_1Sw"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003d2d2abec24b01ae8fe3556220f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bcef54dcb0402d8758e902b4c2b5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "def preprocess_images(image_batch):\n",
    "    # Convert tensor images to PIL images\n",
    "    pil_images = [T.ToPILImage()(img) for img in image_batch]\n",
    "    return processor(images=pil_images, return_tensors=\"pt\")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z59yNHrWOR1k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for batch in testLoader:\n",
    "  tactileColorL, tactileColorR, labels = (\n",
    "      batch['tactileColorL'],\n",
    "      batch['tactileColorR'],\n",
    "      batch['label']\n",
    "  )\n",
    "  input = preprocess_images(tactileColorL)\n",
    "\n",
    "print(input.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0JZCgt7UhVlu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.16422665365143782\n",
      "Epoch 2, Loss: 0.11699135135910992\n",
      "Epoch 3, Loss: 0.10762040791515674\n",
      "Epoch 4, Loss: 0.10240475013386457\n",
      "Epoch 5, Loss: 0.0966916610709288\n",
      "Epoch 6, Loss: 0.09381020727179323\n",
      "Epoch 7, Loss: 0.08632500401982623\n",
      "Epoch 8, Loss: 0.08945765643768633\n",
      "Epoch 9, Loss: 0.10254927168354495\n",
      "Epoch 10, Loss: 0.08683110145282828\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device==\"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "net = MyModel()\n",
    "net.to(torch.float32).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  net.train()\n",
    "  running_loss = 0.0\n",
    "  for batch in trainLoader:\n",
    "    tactileColorL, tactileColorR, labels = (\n",
    "            batch['tactileColorL'],\n",
    "            batch['tactileColorR'],\n",
    "            batch['label']\n",
    "        )\n",
    "    tactileColorL, tactileColorR, labels = (\n",
    "            tactileColorL.to(device),\n",
    "            tactileColorR.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "    inputL = preprocess_images(tactileColorL).to(device)\n",
    "    inputR = preprocess_images(tactileColorR).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(inputL, inputR)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainLoader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy :  0.97125\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in testLoader:\n",
    "        tactileColorL, tactileColorR, labels = (\n",
    "            batch['tactileColorL'],\n",
    "            batch['tactileColorR'],\n",
    "            batch['label']\n",
    "        )\n",
    "        tactileColorL, tactileColorR = tactileColorL.to(device), tactileColorR.to(device)\n",
    "\n",
    "        inputL = preprocess_images(tactileColorL).to(device)\n",
    "        inputR = preprocess_images(tactileColorR).to(device)\n",
    "\n",
    "        outputs = net(inputL, inputR)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy : \", accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch v2.x (Machine Learning)",
   "language": "python",
   "name": "pytorch_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
