{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BonggeunJeon/Follow-up/blob/master/Grasp_Prediction_Bonggeun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial D2: Grasp Prediction\n",
        "\n",
        "## Tasks\n",
        "\n",
        "But first: Copy notebook to the drive (button should be just next to the +Copy +Text)\n",
        "Goal: Train a custom-made neural network to predict whether a grasp will be successful based on two tactile images. (binary classification)\n",
        "\n",
        "### Setup\n",
        "\n",
        "1. Create conda venv and install Tacto on your machine\n",
        "2. Clone\n",
        "```\n",
        "https://github.com/facebookresearch/tacto/\n",
        "```\n",
        "3. Call\n",
        "```\n",
        "pip install -r tacto/requirements/examples.txt\n",
        "```\n",
        "3. Go to **experiments/grasp_stability**\n",
        "4. Call\n",
        "```\n",
        "pip install scipyplot deepdish torch torchvision\n",
        "```\n",
        "\n",
        "### Collect data\n",
        "1. Default script will collects N x 100 samples. How fast is your machine?\n",
        "  \n",
        "  a. It's fast and I have over 100GB free:\n",
        "  ```\n",
        "  python grasp_data_collection.py\n",
        "  ```\n",
        "  b. It is not that fast, and I have less memory:\n",
        "  ```\n",
        "  295    print(\"\\rsample {}\".format(log.id * log.batch_size + len(log.dataList)),end=\"\")\n",
        "  296\n",
        "  297     # print(\"\\rsample {}\".format(log.id), end=\"\")\n",
        "  298\n",
        "  299    if log.id > 2000: # N is 2000, for N = 100 dataset size is ~5.7GB\n",
        "            break\n",
        "  ```\n",
        "\n",
        "  ```\n",
        "  112    data = {\n",
        "  113       \"tactileColorL\": tactileColorL,\n",
        "  114       \"tactileColorR\": tactileColorR,\n",
        "  115       # \"tactileDepthL\": tactileDepthL,\n",
        "  116       # \"tactileDepthR\": tactileDepthR,\n",
        "  117       # \"visionColor\": visionColor,\n",
        "  118      # \"visionDepth\": visionDepth,\n",
        "  119       # \"gripForce\": gripForce,\n",
        "  120       # \"normalForce\": normalForce,\n",
        "  121       \"label\": label,\n",
        "  122   }\n",
        "  ```\n",
        "\n",
        "2. Zip **data** folder and upload it to **Google Drive** or directly\n",
        "\n",
        "## Implement\n",
        "1. MyModel class\n",
        "2. Training loop\n",
        "3. Save model\n",
        "\n",
        "- Use materials from the previous lectures and PyTorch documentation Links:\n",
        "- https://colab.research.google.com/drive/1KyrXe6kErnAYTuFKsRnvDhRLS4pBAiXa?usp=sharing\n",
        "- https://pytorch.org/docs/stable/index.html"
      ],
      "metadata": {
        "id": "PQP1XNUJxbV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "VT5LtUSFyI-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepdish"
      ],
      "metadata": {
        "id": "fXnpe2deFD7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqJKIimRkhbJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import random\n",
        "import os\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import math\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import copy\n",
        "import h5py\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import deepdish as dd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Class and Loader"
      ],
      "metadata": {
        "id": "jYRrHWOEyOON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraspingDataset(Dataset):\n",
        "    def __init__(self, fileNames, fields=[], transform=None, transformDepth=None):\n",
        "        self.transform = transform\n",
        "        self.transformDepth = transformDepth\n",
        "        self.fileNames = fileNames\n",
        "        self.fields = fields + [\"label\"]\n",
        "        self.numGroup = 100  # data points per file\n",
        "\n",
        "        self.dataList = None\n",
        "        self.dataFileID = -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fileNames * self.numGroup)\n",
        "\n",
        "    def load_data(self, idx):\n",
        "        dirName = self.fileNames[idx]\n",
        "        data = {}\n",
        "\n",
        "        for k in self.fields:\n",
        "            fn = dirName.split(\"/\")[-1]\n",
        "\n",
        "            fnk = \"{}_{}.h5\".format(fn, k)\n",
        "\n",
        "            filenamek = os.path.join(dirName, fnk)\n",
        "            d = dd.io.load(filenamek)\n",
        "\n",
        "            data[k] = d\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        fileID = idx // self.numGroup\n",
        "        if fileID != self.dataFileID:\n",
        "            self.dataList = self.load_data(fileID)\n",
        "            self.dataFileID = fileID\n",
        "\n",
        "        sample = {}\n",
        "\n",
        "        data = self.dataList\n",
        "\n",
        "        for k in self.fields:\n",
        "            d = data[k][idx % self.numGroup]\n",
        "\n",
        "            if k in [\"tactileColorL\", \"tactileColorR\", \"visionColor\"]:\n",
        "                d = d[:, :, :3]\n",
        "                # print(k, d.min(), d.max())\n",
        "\n",
        "            if k in [\"tactileDepthL\", \"tactileDepthR\", \"visionDepth\"]:\n",
        "                d = np.dstack([d, d, d])\n",
        "\n",
        "            if k in [\"tactileDepthL\", \"tactileDepthR\"]:\n",
        "                d = d / 0.002 * 255\n",
        "                d = np.clip(d, 0, 255).astype(np.uint8)\n",
        "                # print(\"depth min\", d.min(), \"max\", d.max())\n",
        "\n",
        "            if k in [\"visionDepth\"]:\n",
        "                d = (d * 255).astype(np.uint8)\n",
        "\n",
        "            if k in [\n",
        "                \"tactileColorL\",\n",
        "                \"tactileColorR\",\n",
        "                \"visionColor\",\n",
        "                \"visionDepth\",\n",
        "            ]:\n",
        "                if self.transform:\n",
        "                    d = self.transform(d)\n",
        "\n",
        "            if k in [\n",
        "                \"tactileDepthL\",\n",
        "                \"tactileDepthR\",\n",
        "            ]:\n",
        "                # print(\"before\", d.min(), d.max(), d.mean(), d.std())\n",
        "                d = self.transformDepth(d)\n",
        "                # d = (d + 2) / 0.05\n",
        "                # print(\"after\", d.min(), d.max(), d.mean(), d.std())\n",
        "\n",
        "            sample[k] = d\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "50uPv3QsE8fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that will\n",
        "# - take all samples from rootDir\n",
        "# - divide whole set in N parts\n",
        "# - take i-th for test and rest N-1 for training\n",
        "def load_data(rootDir, K, i):\n",
        "        # K-fold, test the i-th fold, train the rest\n",
        "\n",
        "        fileNames = glob.glob(os.path.join(rootDir, \"*\"))\n",
        "        fileNames = sorted(fileNames)\n",
        "        print(fileNames)\n",
        "\n",
        "        # Split K fold\n",
        "        N = len(fileNames)\n",
        "        n = N // K\n",
        "\n",
        "        idx = list(range(N))\n",
        "        testIdx = idx[n * i : n * (i + 1)]\n",
        "        trainIdx = list(set(idx) - set(testIdx))\n",
        "\n",
        "        trainFileNames = [fileNames[i] for i in trainIdx]\n",
        "        testFileNames = [fileNames[i] for i in testIdx]\n",
        "\n",
        "        trainTransform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomCrop(224),\n",
        "                # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "                # AddGaussianNoise(0.0, 0.01),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        trainTransformDepth = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomCrop(224),\n",
        "                # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.1,), std=(0.2,)),\n",
        "                # AddGaussianNoise(0.0, 0.01),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Create training dataset and dataloader\n",
        "        trainDataset = GraspingDataset(\n",
        "            trainFileNames,\n",
        "            fields=[\"tactileColorL\", \"tactileColorR\"],\n",
        "            transform=trainTransform,\n",
        "            transformDepth=trainTransformDepth,\n",
        "        )\n",
        "        trainLoader = torch.utils.data.DataLoader(\n",
        "            trainDataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        testTransform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomCrop(224),\n",
        "                # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        testTransformDepth = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomCrop(224),\n",
        "                # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.1,), std=(0.2,)),\n",
        "                # AddGaussianNoise(0.0, 0.01),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Create training dataset and dataloader\n",
        "        testDataset = GraspingDataset(\n",
        "            testFileNames,\n",
        "            fields=[\"tactileColorL\", \"tactileColorR\"],\n",
        "            transform=testTransform,\n",
        "            transformDepth=testTransformDepth,\n",
        "        )\n",
        "        testLoader = torch.utils.data.DataLoader(\n",
        "            testDataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        "        )\n",
        "\n",
        "        return trainLoader, testLoader"
      ],
      "metadata": {
        "id": "qxSKpkPtGP6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task I: Get and upload data\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```"
      ],
      "metadata": {
        "id": "3if33sNyyXP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I picked 3rd option\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vZDL4EOH9Mty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: fix path\n",
        "!unzip /content/drive/MyDrive/TutorialD/tutorial-d/data.zip"
      ],
      "metadata": {
        "id": "VPrOzQz9nhgi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Task II: Load Dataset\n",
        "- result: you should see two tactile readings"
      ],
      "metadata": {
        "id": "1wU-023n2iKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: check if your data is in the right dir\n",
        "rootDir = '/content/grasp'\n",
        "\n",
        "# create dataset loaders (do you know why are we using loaders?)\n",
        "trainLoader, testLoader = load_data(rootDir, 10, 0)\n",
        "\n",
        "# show single test example\n",
        "single_test_batch = next(iter(testLoader))\n",
        "tactileColorL, tactileColorR, label = single_test_batch['tactileColorL'], single_test_batch['tactileColorR'], single_test_batch['label']\n",
        "\n",
        "display_reading_index = 0\n",
        "plt.title('Label:' + str(label[display_reading_index].item()))\n",
        "plt.imshow(tactileColorL[display_reading_index,:,:,:].T)\n",
        "plt.show()\n",
        "\n",
        "plt.title('Label:' + str(label[display_reading_index].item()))\n",
        "plt.imshow(tactileColorR[display_reading_index,:,:,:].T)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcLYP8ccGce5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(single_test_batch['tactileColorL'].shape)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sDgyKwMj4JpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task III: Implement custom model and train it\n",
        "- tip 1: all manipulations are performed on tensors, check tensor shape frequently (i.e. print(inputs.shape))\n",
        "- you can use Tutorial A for inspiration https://colab.research.google.com/drive/1KyrXe6kErnAYTuFKsRnvDhRLS4pBAiXa?usp=sharing\n",
        "- one way how it can be done is on slides\n",
        "- for tensor manipulation https://pytorch.org/docs/stable/index.html\n",
        "- take single batch and play with it"
      ],
      "metadata": {
        "id": "1T7hrAaF_CL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(testLoader))"
      ],
      "metadata": {
        "id": "sXbyoFcH4U-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xqq8AFMA6u1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement network\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n",
        "        self.vit.classifier = nn.Identity()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.vit.config.hidden_size * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, tactileColorL, tactileColorR):\n",
        "      featuresL = self.vit(pixel_values=tactileColorL).logits\n",
        "      featuresR = self.vit(pixel_values=tactileColorR).logits\n",
        "      combined_features = torch.cat((featuresL, featuresR), dim=1)\n",
        "      output = self.classifier(combined_features)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "PxAr3J9oMwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "\n",
        "print(next(model.parameters()).dtype)"
      ],
      "metadata": {
        "id": "9T1H2UF2NeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "def preprocess_images(image_batch):\n",
        "    # Convert tensor images to PIL images\n",
        "    pil_images = [T.ToPILImage()(img) for img in image_batch]\n",
        "    return processor(images=pil_images, return_tensors=\"pt\")['pixel_values']"
      ],
      "metadata": {
        "id": "adi7kMQO_1Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in testLoader:\n",
        "  tactileColorL, tactileColorR, labels = (\n",
        "      batch['tactileColorL'],\n",
        "      batch['tactileColorR'],\n",
        "      batch['label']\n",
        "  )\n",
        "  input = preprocess_images(tactileColorL)\n",
        "\n",
        "print(input.dtype)"
      ],
      "metadata": {
        "id": "Z59yNHrWOR1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = MyModel()\n",
        "net.to(torch.float32).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  net.train()\n",
        "  running_loss = 0.0\n",
        "  for batch in trainLoader:\n",
        "    tactileColorL, tactileColorR, labels = (\n",
        "            batch['tactileColorL'],\n",
        "            batch['tactileColorR'],\n",
        "            batch['label']\n",
        "        )\n",
        "    tactileColorL, tactileColorR, labels = (\n",
        "            tactileColorL.to(device),\n",
        "            tactileColorR.to(device),\n",
        "            labels.to(device)\n",
        "        )\n",
        "\n",
        "    inputL = preprocess_images(tactileColorL).to(device)\n",
        "    inputR = preprocess_images(tactileColorR).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputL, inputR)\n",
        "    loss = loss_function(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainLoader)}\")\n"
      ],
      "metadata": {
        "id": "0JZCgt7UhVlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}